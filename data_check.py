import time
import json
import os
import glob
import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import argparse

prompt = '''I will give you a question, the ground truth answer and one answers generated by another model. You need to judge whether the answer is correct or not based on the ground truth answer. As long as the answer has the same meaning with the ground truth answer, even if the format is not completely the same, it is correct. For each question, if the answer is correct, just return 1. If the answer is incorrect, return 0. You should only return 1 or 0 for each question. The output should be a list containing your judgement of the answer. Here is an example:
——Example 1——
Question: <image>\nWhat company held 15 percent of the microwave oven market in the United States in 2008?\nAnswer the question using a single word or phrase.
Ground Truth: Samsung.
Model 1: Samsung and Sharp held 15 percent of the microwave oven market share in the United States in 2008.
Your answer: [0]
——Example 2——
Question: <image>\nWhat company held 15 percent of the microwave oven market in the United States in 2008?\nAnswer the question using a single word or phrase.
Model 1: Samsung held 15 percent of the microwave oven market in the United States in 2008.
Your answer: [1]

Now please generate your answers based on the given questions, ground truth answers and model answers. Please note that you should only output the list of your answers, strictly follow the format.   
'''

data_paths = ["data.json"]

LLM_PATH = "PATH/TO/LLM"
tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)

sampling_params = SamplingParams(temperature=0.2, top_p=0.7, repetition_penalty=1.05, max_tokens=512)

llm = LLM(model=LLM_PATH,tensor_parallel_size=1)

batch_size = 32

for data_path in data_paths:
    with open(data_path, "r") as f:
        data = json.load(f)

    all_inputs = []
    bad_count = 0
    for idx, data_line in enumerate(data):
        question = data_line['question']
        ground_truth = data_line['conversations'][1]['value']
        response = []
        for meta_ in meta:
            # check whether we can parse the response of reasoning steps
            response_ = data_line['steps'][-1]
            try:
                response.append(json.loads(response_[1])['content'])
            except:
                bad_count += 1
                response.append("No available answer")

        user_input = f"Question: {question}\nGround Truth: {ground_truth}\nModel 1: {response[0]}\nYour answer: "
        
        messages = [
            {"role": "system", "content": f"You are a helpful assistant.\n{prompt}"},
            {"role": "user", "content": user_input}
        ]
        all_inputs.append({
            "raw": data_line,
            "message": messages
        })

    print(bad_count)

    for i in tqdm.tqdm(range(0, len(all_inputs), batch_size)):
        batch = all_inputs[i:i+batch_size]
        conversations = []
        batch_raws, batch_messages = [batch[i]["raw"] for i in range(len(batch))], [batch[i]["message"] for i in range(len(batch))]
        for messages in batch_messages:
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            conversations.append(text)

        outputs = llm.generate(conversations, sampling_params, use_tqdm=False)
        for j, output in enumerate(outputs):
            generated_text = output.outputs[0].text
            generated_text = generated_text.split("]")[0] + ']'
            try:
                all_inputs[i+j]['answer'] = json.loads(generated_text)
            except:
                print("Error")
                all_inputs[i+j]['answer'] = [0]

    save_path = ""
    with open(save_path, "w") as f:
        f.write(json.dumps(all_inputs))
